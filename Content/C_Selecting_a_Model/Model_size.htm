<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Model size vs hardware capabilities</h1>
        <MadCap:snippetBlock src="../Resources/Snippets/SkipAhead.flsnp" />
        <MadCap:snippetBlock src="../Resources/Snippets/SysReqReminder.flsnp" />
        <p>To get started with selecting your first model, open huggingface.co in your browser, and navigate to <b>Models</b> &gt;&#160;<b>Natural Language Processing</b> &gt;&#160;<b>Text Generation</b>.</p>
        <p>You will see a long list of currently available models. You may click on any given model to see a <b>model card</b>, or a brief overview of the model's features, terms of use (not all models are free to use commercially) and recommended <a href="../D_Model_configuration/Model_parameters.htm">template or settings.</a></p>
        <p>While browsing through, notice that most model names contain a number followed by the letter B. This denotes the number of billions of parameters contained by the given model. For instance, <b>Llama-2-70b-chat</b> was trained on 70 billion parameters. </p>
        <p>
            <img src="IMG/HF_Models_Size.png" alt="Image showing examples of models on huggingface.co" style="width: auto;height: auto;" class="thumbnail" />
        </p>
        <p>You will also notice some models with 8x7B in their names - these are Mixture of Experts models - in essence, 8 smaller models working in conjunction.</p>
        <p>To put things simply - the larger this number, the 'smarter' the model will be. However, larger models, while smart, have a downside - using them requires a powerful computer. <b>The larger the 'B' number, the larger the memory requirement.</b> </p>
        <h2>Quantization </h2>
        <p>Further complicating this calculation is the matter of <b>quantization</b>, or model '<i>quants</i>'. For simplicity's sake, just understand that the model can be compressed to more easily fit into your memory - however, it will be slightly less 'smart' than an uncompressed model.</p>
        <p>Some model authors provide a handy table showing exactly how much memory you will need to comfortably run a given model. In this example, a Q8 quant of <i>codellama-70b</i> can fit in 76GB&#160;of RAM, but a Q5 quant will fit into 50GB&#160;of memory.</p>
        <p>
            <img src="IMG/HF_Quant_table.png" alt="Image showing an example quantification table" style="width:auto;height:auto;" class="thumbnail" />
        </p>
        <div class="tip">
            <p> Tip</p>
            <p>It's generally agreed that it's better to run a larger model quantized, than to run a smaller but unquantized model. It's also agreed that it's best not to go below Q5.</p>
        </div>
        <p>You can very roughly assume that one billion of parameters will fit, when quantized, into slightly more than 1GB&#160;of RAM or VRAM. Remember that you will need to leave some memory available for your operating system and the LLM's context. </p>
        <p>Using this information you should be able to roughly estimate how large of a model you can comfortably run on your machine. One sensible approach would be to begin with a small model, for instance a 7B or 13B one, and if it runs well, move up to larger ones. For now, browse through and see if anything catches your eye.</p>
        <div class="next">Proceed to learning about <a href="Model_formats.htm">model formats</a></div>
        <div class="next">Alternatively, try out one of the <a href="Recommended_first_models.htm">recommended models</a></div>
    </body>
</html>