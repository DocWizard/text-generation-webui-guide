<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>GGUF model format - downloading a model</h1>
        <MadCap:snippetBlock src="../Resources/Snippets/SkipAhead.flsnp" MadCap:conditions="General.Web only" />
        <MadCap:snippetBlock src="../Resources/Snippets/SysReqReminder.flsnp" />
        <div>
            <p>Machine learning is a dynamic field, and there are many standards and quantization formats, both current and already obsolete. For the sake of this guide, we will consider the most versatile of them: GGUF.</p>
        </div>
        <div>
            <p><b>GGUF</b>&#160;is a file format used by one of the most popular back-end libraries, <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a>. Models quantized using this format are capable of being loaded in RAM. Part of the model can also be offloaded to VRAM for extra overall speed. Thanks to this, GGUF&#160;remains one of the most popular model formats in use today. Models quantized in this format are usually denoted with a -GGUF&#160;suffix on <a href="http://huggingface.co/" target="_blank">huggingface.co</a>:</p>
            <p>
                <img src="IMG/HF_GGUF_example.png" style="width: auto;height: auto;" alt="Image showing an example of a GGUF model" class="thumbnail" />
            </p>
            <p>You can also use the filter function to limit the displayed models to  this format only:</p>
            <p>
                <img src="IMG/HF_GGUF_search.png" alt="Image showing the position of the filter bar on HuggingFace" style="width: auto;height: auto;" class="thumbnail" />
            </p>
            <p>Browse through the available models and find one that suits your needs. Take into account the amount of memory you will need to load the model.</p>
            <div class="tip">
                <p style="font-weight: bold;">Tip:                </p>
                <p>Remember, you can start with a small, 7B model and upgrade to a larger one if it turns out your computer can handle it. Refer to the memory requirements table provided by most GGUF quant uploaders.</p>
                <p>Keep in mind that quants below Q5 are generally not recommended. Try to pick a Q5_K_M quant to achieve the best size/quality ratio.</p>
            </div>
            <p>When you've picked a model and the associated quant size, from the <b>model card</b> go to <b>Files and versions</b>, then select the quant you wish to download. After the download is finished, move the <i>.gguf</i> file to your text-generation-webui directory, dropping it in the <b>models</b> folder.</p>
        </div>
        <div class="next" MadCap:conditions="General.Web only">When you've selected and downloaded a model, proceed to <a href="../D_Model_configuration/Model_configuration.htm">model loading and configuration</a></div>
        <div class="next" MadCap:conditions="General.Web only">If you still can't decide, try one of the <a href="Recommended_first_models.htm">recommended models</a></div>
    </body>
</html>