<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>GGUF model format - downloading a model</h1>
        <div>
            <p>Machine learning is a dynamic field, and there are many standards and quantization formats, both current and already obsolete. For the sake of this guide, we will consider the most versatile of them - GGUF.</p>
        </div>
        <MadCap:snippetBlock src="../Resources/Snippets/SysReqReminder.flsnp" />
        <MadCap:snippetBlock src="../Resources/Snippets/SkipAhead.flsnp" />
        <div>
            <p><b>GGUF</b>&#160;is a popular file format used by one of the most popular backend libraries, <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a>. Models quantized using this format are capable of being loaded in RAM. Part of the model can also be offloaded to VRAM for extra overall speed. Thanks to this, GGUF&#160;remains one of the most popular model formats in use today. Models quantized in this format are usually denoted with a -GGUF&#160;suffix on huggingface.co:</p>
            <p>
                <img src="IMG/HF_GGUF_example.png" style="width: auto;height: auto;" alt="Image showing an example of a GGUF model" class="thumbnail" />
            </p>
            <p>You may also use the filter function limit the displayed models to only this format:</p>
            <p>
                <img src="IMG/HF_GGUF_search.png" alt="Image showing the position of the filter bar on HuggingFace" style="width: auto;height: auto;" class="thumbnail" />
            </p>
            <p>Browse through the available models and try to find one that suits your needs, taking into account the amount of memory you will need to load the model. <i>TheBloke</i> and <i>bartowski</i> are two HuggingFace users who consistently upload GGUF&#160;quants of popular models.</p>
            <div class="tip">
                <p>Tip:</p>
                <p>Remember, you can start with a small, 7B model and upgrade to a larger one if it turns out your computer can handle it. Refer to the memory requirements table provided by most GGUF quant uploaders.</p>
                <p>Keep in mind that quants below Q5 are generally not recommended. Try to pick a Q5_K_M quant to achieve the best size/quality ratio.</p>
            </div>
            <p>Once you've picked a model and the associated quant size, from the <b>model card</b> navigate to <b>Files and versions</b>, then simply click on the quant you wish to download. Once the download is finished, move the .gguf file to your text-generation-webui directory, dropping it in the <b>models</b> folder.</p>
        </div>
        <div class="next">Once you've selected and downloaded a model, proceed to <a href="../D_Model_configuration/Model_configuration.htm">model loading and configuration</a></div>
        <div class="next">If you still can't decide, try out one of the <a href="Recommended_first_models.htm">recommended models</a></div>
    </body>
</html>