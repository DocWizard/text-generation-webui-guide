<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Model parameters</h1>
        <p>Unlike online services such as ChatGPT, Gemma or Claude, local LLMs offer you the possibility of manually adjusting their parameters. You can make the AI's responses more or less random and predictable, change the output length and more.</p>
        <h2>Instruction templates</h2>
        <p>Instruction templates are a template for the format in which your prompts should be sent to the AI. After loading a model in text-generation-webui, go to the <b>Parameters</b> &gt;&#160;<b>Instruction template </b> tab</p>
        <p>Usually, the model's author includes the required template in the model's metadata, allowing text-generation-webui to automatically set the correct template. If this doesn't happen, you will need to locate the recommended template's name in the model's card on <a href="http://huggingface.co/" target="_blank">huggingface.co</a>. </p>
        <p>Setting the template manually is simple, as text-generation-webui comes with all the most often used templates pre-installed. Select the desired template from the drop-down menu, and confirm with <b>"Load"</b>. You're done!</p>
        <h2>Generation parameters</h2>
        <p>Model generation parameters allow you more freedom to experiment with your local model. After loading a model, go to <b>Parameters</b> &gt;&#160;<b>Generation.</b> You will see the following interface:</p>
        <p>
            <img src="IMG/ooba-gen-params.png" alt="Image showing the Generation parameters tab in text-generation-webui" style="width: auto; height: auto;" class="thumbnail" />
        </p>
        <ol>
            <li><b>Premade presets</b> - For Chat-GPT-like generation, LLaMA-Precise is recommended. Simply select the preset from the drop-down menu.</li>
            <li><b>max_new_tokens</b> - controls the length of the model's output.</li>
            <li><b>temperature</b> - influences the model's "creativity". The higher this number, the more randomness will be introduced to the token selection. Keep this number between 0.7 and 1.2. For general AI&#160;assistant chats, the lower end of this spectrum is recommended.</li>
            <li><b>top_p</b> - cuts off the least probable next tokens during the generation. A&#160;top_p of 0.1 means that the tokens comprising the top 10% of probability are considered.</li>
            <li><b>Repetition penalty</b> - change this number if your model keeps repeating itself or using the same words too often. Between 1.1 and 1.2 is best.</li>
        </ol>
        <p>Feel free to experiment with these options and see how the model's responses change!</p>
        <div class="next" MadCap:conditions="General.Web only">Next, you can move on to <a href="../E_Chatting/Chatting.htm">chatting with the local model</a></div>
    </body>
</html>